{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import bernoulli,  norm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class binary_naive_bayes():\n",
    "    \"\"\"\n",
    "    A class which can fit data set according to the Naive Bayes method and predict new unseen data\n",
    "    (USING MLE)\n",
    "    Atributes:\n",
    "        data : Contains the Data which the model was fit with\n",
    "    Methods:\n",
    "        fit(data)\n",
    "        predit(new_data)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the class with data, model and Class atributes\"\"\"\n",
    "        self.data = None\n",
    "        self.model = {'discrete_feature_MLE' : [], 'continuous_feature_MLE' : []}\n",
    "        self.classes = [0, 1] #Note that by making this dynamic we can take this from a binary classifyer to an arbitary one\n",
    "    def fit(self, discrete_features, continuous_features, output):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            fit(data): Creates a model for prediction\n",
    "        Parameters:\n",
    "            discrete_features: A $nxm_0$ numpy array which contains the data to train on \n",
    "                              (m is number of discrete_features n is the number of entries)\n",
    "            continuous_features: A $nxm_1$ numpy array which contains the data to train on \n",
    "                                 ($m_1$ is number of discrete_features n is the number of entries)\n",
    "            output: A $nx1$ numpy array which contains either 0 or 1 for the output of the data given\n",
    "        \"\"\"\n",
    "        for i in range(continuous_features.shape[1]):\n",
    "            for _class in self.classes:\n",
    "                col = continuous_features[np.where(output == _class)[0]][:, i]\n",
    "                self.model['continuous_feature_MLE'].append((_class, np.mean(col), np.std(col, ddof=1)))\n",
    "        for i in range(discrete_features.shape[1]):\n",
    "            for _class in self.classes:\n",
    "                col = discrete_features[np.where(output == _class)[0]][:, i]\n",
    "                self.model['discrete_feature_MLE'].append((_class, np.mean(col)))\n",
    "        self.data = np.hstack((discrete_features, continuous_features))\n",
    "    def predict(self, new_data_discrete_features, new_data_continuous_features):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            predict(new_data): Predicts which of the two categories `new_data` will fall into\n",
    "                               Raises Values error if no data has been fit yet\n",
    "        Parameters:\n",
    "            new_data_discrete_features: A new nxm_0 numpy vector which contains all of the atributes \n",
    "            new_data_continuous_features: A new nxm_1 numpy vector which contains all of the atributes \n",
    "            \n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"You must successfully fit data before calling this method\")\n",
    "        if new_data_discrete_features.shape[0] != new_data_continuous_features.shape[0]:\n",
    "            raise ValueError(\"Discrete and continous features don't match in the first dimension\")\n",
    "        def get_prob_for_single(new_data_discrete_feature, new_data_continuous_feature):\n",
    "            prob = []\n",
    "            for _class in self.classes:\n",
    "                class_prob = 0\n",
    "                discrete_features = [j for j in self.model['discrete_feature_MLE'] if j[0]==_class]\n",
    "                continuous_features = [j for j in self.model['continuous_feature_MLE'] if j[0]==_class]\n",
    "                for i, discrete_params in enumerate(discrete_features):\n",
    "                    class_prob += np.log(bernoulli.pmf(new_data_discrete_feature[i], discrete_params[1]))\n",
    "                for i, continuous_params in enumerate(continuous_features):\n",
    "                    _, mean, std = continuous_params\n",
    "                    class_prob += np.log(norm.pdf(new_data_continuous_feature[i], mean, std))\n",
    "                prob.append(class_prob)\n",
    "            prob = np.exp(prob)\n",
    "            return prob/(np.sum(prob)) #Scale\n",
    "        prob_array = []\n",
    "        for k in range(new_data_discrete_features.shape[0]):\n",
    "            prob_array.append(get_prob_for_single(new_data_discrete_features[k, :], new_data_continuous_features[k, :]))\n",
    "        return np.array(prob_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[See adult data set here -- classification problem to determine if individual makes over 50K a year](https://archive.ics.uci.edu/ml/datasets/Adult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lnelson/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./adult.data', delimiter=\", \")#Import data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[[\"output\"]]\n",
    "categorical_cols = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country'] \n",
    "X = data.drop(['fnlwgt', 'output'], axis=1)\n",
    "X = pd.get_dummies(X, columns = categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_var_column_names = [column_name for column_name in X.columns if \"_\" in column_name]\n",
    "continuous_var_column_names = [column_name for column_name in X.columns if \"_\" not in column_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_discrete = X[discrete_var_column_names].to_numpy()\n",
    "X_continuous = X[continuous_var_column_names].to_numpy()\n",
    "y = (y==\"<=50K\").to_numpy() + 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 5)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_continuous.shape \n",
    "#We're going to stack our X_discrete and X_continuous -- so we can test, train, split -- \n",
    "#we'll just tack the continous variables at the end and remember to break it back up after the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.hstack((X_discrete, X_continuous)), y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_discrete_trian = X_train[:, :102]\n",
    "X_continuous_trian = X_train[:, 102:107]\n",
    "\n",
    "personal_inmplementaiton_clf = binary_naive_bayes()\n",
    "personal_inmplementaiton_clf.fit(X_discrete_trian, X_continuous_trian, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lnelson/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:65: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/lnelson/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:62: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/lnelson/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:68: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "X_discrete_test = X_test[:, :102]\n",
    "X_continuous_test = X_test[:, 102:107]\n",
    "\n",
    "y_pred = personal_inmplementaiton_clf.predict(X_discrete_test, X_continuous_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8305415968732551"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that compared to the out of the Box Naive Bayes provided by sklearn will actually perform worse then the model above. This is because it (perhaps wrongly) assumes that even the discrete data is drawn from a normal distrobution. Thus arguably the above model above is `more` correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lnelson/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.801321421924437"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
